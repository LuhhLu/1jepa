{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yaml\n",
    "from app.vjepa.utils import (\n",
    "    load_checkpoint_cpu,\n",
    "    init_video_model,\n",
    "    init_opt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'one_bit_logs/params-pretrain.yaml'\n",
    "with open(fname, 'r') as y_file:\n",
    "        params = yaml.load(y_file, Loader=yaml.FullLoader)\n",
    "\n",
    "args=params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- META\n",
    "cfgs_meta = args.get('meta')\n",
    "save_every_freq = cfgs_meta.get('save_every_freq', -1)\n",
    "skip_batches = cfgs_meta.get('skip_batches', -1)\n",
    "use_sdpa = cfgs_meta.get('use_sdpa', False)\n",
    "which_dtype = cfgs_meta.get('dtype')\n",
    "if which_dtype.lower() == 'bfloat16':\n",
    "    dtype = torch.bfloat16\n",
    "    mixed_precision = True\n",
    "elif which_dtype.lower() == 'float16':\n",
    "    dtype = torch.float16\n",
    "    mixed_precision = True\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "    mixed_precision = False\n",
    "\n",
    "# -- MASK\n",
    "cfgs_mask = args.get('mask')\n",
    "\n",
    "# -- MODEL\n",
    "cfgs_model = args.get('model')\n",
    "model_name = cfgs_model.get('model_name')\n",
    "pred_depth = cfgs_model.get('pred_depth')\n",
    "pred_embed_dim = cfgs_model.get('pred_embed_dim')\n",
    "uniform_power = cfgs_model.get('uniform_power', True)\n",
    "use_mask_tokens = cfgs_model.get('use_mask_tokens', True)\n",
    "zero_init_mask_tokens = cfgs_model.get('zero_init_mask_tokens', True)\n",
    "\n",
    "# -- DATA\n",
    "cfgs_data = args.get('data')\n",
    "dataset_type = cfgs_data.get('dataset_type', 'videodataset')\n",
    "mask_type = cfgs_data.get('mask_type', 'multiblock3d')\n",
    "dataset_paths = cfgs_data.get('datasets', [])\n",
    "datasets_weights = cfgs_data.get('datasets_weights', None)\n",
    "if datasets_weights is not None:\n",
    "    assert len(datasets_weights) == len(dataset_paths), 'Must have one sampling weight specified for each dataset'\n",
    "batch_size = cfgs_data.get('batch_size')\n",
    "num_clips = cfgs_data.get('num_clips')\n",
    "num_frames = cfgs_data.get('num_frames')\n",
    "tubelet_size = cfgs_data.get('tubelet_size')\n",
    "sampling_rate = cfgs_data.get('sampling_rate')\n",
    "duration = cfgs_data.get('clip_duration', None)\n",
    "crop_size = cfgs_data.get('crop_size', 224)\n",
    "patch_size = cfgs_data.get('patch_size')\n",
    "pin_mem = cfgs_data.get('pin_mem', False)\n",
    "num_workers = cfgs_data.get('num_workers', 1)\n",
    "filter_short_videos = cfgs_data.get('filter_short_videos', False)\n",
    "decode_one_clip = cfgs_data.get('decode_one_clip', True)\n",
    "log_resource_util_data = cfgs_data.get('log_resource_utilization', False)\n",
    "\n",
    "# -- DATA AUGS\n",
    "cfgs_data_aug = args.get('data_aug')\n",
    "ar_range = cfgs_data_aug.get('random_resize_aspect_ratio', [3/4, 4/3])\n",
    "rr_scale = cfgs_data_aug.get('random_resize_scale', [0.3, 1.0])\n",
    "motion_shift = cfgs_data_aug.get('motion_shift', False)\n",
    "reprob = cfgs_data_aug.get('reprob', 0.)\n",
    "use_aa = cfgs_data_aug.get('auto_augment', False)\n",
    "\n",
    "# -- LOSS\n",
    "cfgs_loss = args.get('loss')\n",
    "loss_exp = cfgs_loss.get('loss_exp')\n",
    "reg_coeff = cfgs_loss.get('reg_coeff')\n",
    "\n",
    "# -- OPTIMIZATION\n",
    "cfgs_opt = args.get('optimization')\n",
    "ipe = cfgs_opt.get('ipe', None)\n",
    "ipe_scale = cfgs_opt.get('ipe_scale', 1.0)\n",
    "clip_grad = cfgs_opt.get('clip_grad', None)\n",
    "wd = float(cfgs_opt.get('weight_decay'))\n",
    "final_wd = float(cfgs_opt.get('final_weight_decay'))\n",
    "num_epochs = cfgs_opt.get('epochs')\n",
    "warmup = cfgs_opt.get('warmup')\n",
    "start_lr = cfgs_opt.get('start_lr')\n",
    "lr = cfgs_opt.get('lr')\n",
    "final_lr = cfgs_opt.get('final_lr')\n",
    "ema = cfgs_opt.get('ema')\n",
    "betas = cfgs_opt.get('betas', (0.9, 0.999))\n",
    "eps = cfgs_opt.get('eps', 1.e-8)\n",
    "\n",
    "# -- LOGGING\n",
    "cfgs_logging = args.get('logging')\n",
    "folder = cfgs_logging.get('folder')\n",
    "tag = cfgs_logging.get('write_tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:MultiMaskWrapper(\n",
      "  (backbone): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed3D(\n",
      "      (proj): BitConv3d(3, 1024, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-23): 24 x Block(\n",
      "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): BitLinear(in_features=1024, out_features=3072, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): BitLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (fc1): BitLinear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): BitLinear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "INFO:root:PredictorMultiMaskWrapper(\n",
      "  (backbone): VisionTransformerPredictor(\n",
      "    (predictor_embed): Linear(in_features=1024, out_features=384, bias=True)\n",
      "    (mask_tokens): ParameterList(\n",
      "        (0): Parameter containing: [torch.float32 of size 1x1x384 (cuda:0)]\n",
      "        (1): Parameter containing: [torch.float32 of size 1x1x384 (cuda:0)]\n",
      "    )\n",
      "    (predictor_blocks): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): BitLinear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): BitLinear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (fc1): BitLinear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): BitLinear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (predictor_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "    (predictor_proj): Linear(in_features=384, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "INFO:root:Encoder number of parameters: 303885312\n",
      "INFO:root:Predictor number of parameters: 22082944\n"
     ]
    }
   ],
   "source": [
    "encoder, predictor = init_video_model(\n",
    "    uniform_power=uniform_power,\n",
    "    use_mask_tokens=use_mask_tokens,\n",
    "    num_mask_tokens=len(cfgs_mask),\n",
    "    zero_init_mask_tokens=zero_init_mask_tokens,\n",
    "    device=device,\n",
    "    patch_size=patch_size,\n",
    "    num_frames=num_frames,\n",
    "    tubelet_size=tubelet_size,\n",
    "    model_name=model_name,\n",
    "    crop_size=crop_size,\n",
    "    pred_depth=pred_depth,\n",
    "    pred_embed_dim=pred_embed_dim,\n",
    "    use_sdpa=use_sdpa,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:loaded pretrained encoder from epoch 302 with msg: <All keys matched successfully>\n",
      "INFO:root:loaded pretrained predictor from epoch 302 with msg: <All keys matched successfully>\n",
      "INFO:root:Encountered exception when loading checkpoint 'NoneType' object has no attribute 'load_state_dict'\n"
     ]
    }
   ],
   "source": [
    "load_path = 'one_bit_logs/jepa-latest.pth.tar'\n",
    "(\n",
    "encoder,\n",
    "predictor,\n",
    "target_encoder,\n",
    "opt,\n",
    "scaler,\n",
    "epoch,\n",
    ") = load_checkpoint_cpu(r_path=load_path,encoder=encoder,predictor=predictor, target_encoder=None, opt = None, scaler= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 假设您的encoder已经定义好并加载了权重\n",
    "conv3d_weight = encoder.backbone.patch_embed.proj.weight.data.cpu().numpy()\n",
    "\n",
    "def save_kernels_as_images(conv3d_weight, grid_size=20):\n",
    "    out_channels, in_channels, T, H, W = conv3d_weight.shape\n",
    "    \n",
    "    # 创建输出文件夹\n",
    "    output_dir = \"conv3d_kernels\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    num_kernels_to_plot = min(out_channels * T, grid_size * grid_size)\n",
    "    \n",
    "    # 图像大小\n",
    "    img_height = H\n",
    "    img_width = W\n",
    "\n",
    "    # 创建一个大的画布\n",
    "    grid_img = np.zeros((grid_size * img_height, grid_size * img_width, 3))\n",
    "\n",
    "    kernel_idx = 0\n",
    "    for out_ch in range(out_channels):\n",
    "        for t in range(T):\n",
    "            if kernel_idx >= num_kernels_to_plot:\n",
    "                break\n",
    "\n",
    "            kernel = conv3d_weight[out_ch]\n",
    "            kernel_slice = kernel[:, t, :, :]  # 取出时间维度t的切片\n",
    "\n",
    "            # 归一化\n",
    "            kernel_slice = (kernel_slice - kernel_slice.min()) / (kernel_slice.max() - kernel_slice.min())\n",
    "            kernel_slice = np.transpose(kernel_slice, (1, 2, 0))  # 将通道放到最后\n",
    "\n",
    "            row = kernel_idx // grid_size\n",
    "            col = kernel_idx % grid_size\n",
    "\n",
    "            grid_img[row * img_height:(row + 1) * img_height, col * img_width:(col + 1) * img_width, :] = kernel_slice\n",
    "\n",
    "            kernel_idx += 1\n",
    "\n",
    "    plt.figure(figsize=(grid_size, grid_size))\n",
    "    plt.imshow(grid_img)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(output_dir, 'kernels_grid.png'))\n",
    "    plt.close()\n",
    "\n",
    "save_kernels_as_images(conv3d_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from src.models.utils.bit_conv import weight_quant\n",
    "\n",
    "# 假设您的encoder已经定义好并加载了权重\n",
    "conv3d_weight = weight_quant(encoder.backbone.patch_embed.proj.weight.data.cpu()).numpy()\n",
    "\n",
    "\n",
    "def save_kernels_as_images(conv3d_weight, grid_size=20):\n",
    "    out_channels, in_channels, T, H, W = conv3d_weight.shape\n",
    "    \n",
    "    # 创建输出文件夹\n",
    "    output_dir = \"conv3d_kernels\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    num_kernels_to_plot = min(out_channels * T, grid_size * grid_size)\n",
    "    \n",
    "    # 图像大小\n",
    "    img_height = H\n",
    "    img_width = W\n",
    "\n",
    "    # 创建一个大的画布\n",
    "    grid_img = np.zeros((grid_size * img_height, grid_size * img_width, 3))\n",
    "\n",
    "    kernel_idx = 0\n",
    "    for out_ch in range(out_channels):\n",
    "        for t in range(T):\n",
    "            if kernel_idx >= num_kernels_to_plot:\n",
    "                break\n",
    "\n",
    "            kernel = conv3d_weight[out_ch]\n",
    "            kernel_slice = kernel[:, t, :, :]  # 取出时间维度t的切片\n",
    "\n",
    "            # 归一化\n",
    "            kernel_slice = (kernel_slice - kernel_slice.min()) / (kernel_slice.max() - kernel_slice.min())\n",
    "            kernel_slice = np.transpose(kernel_slice, (1, 2, 0))  # 将通道放到最后\n",
    "\n",
    "            row = kernel_idx // grid_size\n",
    "            col = kernel_idx % grid_size\n",
    "\n",
    "            grid_img[row * img_height:(row + 1) * img_height, col * img_width:(col + 1) * img_width, :] = kernel_slice\n",
    "\n",
    "            kernel_idx += 1\n",
    "\n",
    "    plt.figure(figsize=(grid_size, grid_size))\n",
    "    plt.imshow(grid_img)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(output_dir, 'kernels_grid_one_bit.png'))\n",
    "    plt.close()\n",
    "\n",
    "save_kernels_as_images(conv3d_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
